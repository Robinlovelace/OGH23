---
title: "Raster and vector data cubes in R"
author: "Edzer Pebesma"
date: "Aug 23, 2023"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
---

# Phenomena: spatially discreet or continuous?

[Stevens' 1946 "On the Theory of Scales of
Measurement"](https://www.science.org/doi/10.1126/science.103.2684.677)
says there are four measurement scales: nominal, ordinal, interval
and ratio. We can simplify that to discrete (nominal, ordinal) and
continuous (interval, ratio). Are the phenomena we study discreet or
continuous?

We think of our phenomenon as $Z(s)$, property $Z$ is measured on object
$s_i$ or at location $s$.  For making progress, we need to
distinguish here whether we talk about

* (non-spatial) properties of objects or locations: $Z$
    * your blood pressure and temperature are two continuous variables on a discrete object (you)
    * your DNA (or at least, the presence of a specific gene) is discrete
* spatial continuity or discreteness: $s$
    * body temperature is continuous within your body, but does not extend it; average body temperature is a property of a spatially discrete entity
    * air temperature is continuous through the air, and is spatially continuous (in $s$), as well as a continuous property (in $Z$)
    * soil type or land use type is continuous in space $s$, but discrete in value $Z$

In space-time we have for instance:
    * temperature is continuous in space, time and property
    * being infected with disease $x$ is discrete in space (e.g., a person) but continuous in time.
    * a remote sensing image is (semi-) continuous in space $s$ and value $Z$ (energy) but the _observation_ is discrete in time (snapshot); the energy phenomena is continuous in time

We often associate

* ***vector data*** with _spatially_ discrete phenomena ("features")
* ***raster data*** with _spatially_ continuous phenomena (fields, coverages)

but this is often not right, e.g.  POINTs can

* reflect locations of things (trees, persons, cars)
* locations for _measurements_ of spatially continuous phenomena (air temperature, air quality)

LINESTRINGs can

* reflect roads, railways or borders
* reflect contour lines, which are discretely sampled representations of a spatially continuous elevation

POLYGONs can

* reflect administrative regions, voting districts or forest stands
* reflect the boundaries of of a spatially continuous discrete variable like land use or soil type

Linestring and polygon geometries are collections of points, and attributes associated with the geometry can be
associated either with

* all of the points (1 value: population count, population density, size or length)
* each of the points (soil type, land use, contour line elevation)

We say for the first relation that the attribute has
_block support_, and for the second that it has _point
support_, following the geostatistical literature. [Further
reading](https://r-spatial.org/book/05-Attributes.html).

Getting this wrong, e.g. when downscaling polygons, may be a source
of gross error in subsequent analysis. Before you analyse data,
it makes a lot of sense to not only consider the measurement scale
of your attribute $Z$, but also whether it is spatially continuous
or discrete ($s$). If it is discrete, counting, sums and spatial
densities make sense, if it is continuous averaging and interpolation
(prediction over continuous space) make sense.

# Tidy data?

Recall [tidy data](https://www.jstatsoft.org/article/view/v059i10):
_each variable is a column, each observation is a row, and each
type of observational unit is a table._

```{r}
head(mtcars)
```

`sf` and `geopandas` objects belong to this category.

## Enter raster data and data cubes

[See here](https://r-spatial.org/book/06-Cubes.html)

## Limits of tables

For raster data, we can, obviously, put things in tables, in
different ways

```{r}
library(stars)
L7_ETMs |> as.data.frame() |> head()
L7_ETMs |> st_as_sf(long = TRUE, as_points = TRUE) |> head()
L7_ETMs |> st_as_sf(as_points = TRUE) |> head()
L7_ETMs |> st_as_sf(as_points = FALSE) |> head()
```

In principle, everything you can do with arrays, you can do with
tables; not vice versa. Why then would you work with arrays?

## Why/when would you stick to arrays?

If you use arrays, you have the advantage of:
	* guarantee of complete data: "empty" fields are filled with 0 (count) or NA (continuous, or nominal)
    * there is no need to take care of completeness, data are complete by construction
	* in-memory arrays have a continous memory layout, hence indexes are for free: we "know" where each data value is ("[geotransform](https://r-spatial.org/book/06-Cubes.html#regular-dimensions-gdals-geotransform)")
    * for larger datasets: arrays form an easy and useful abstraction
    * array [_operations_](https://r-spatial.org/book/06-Cubes.html#sec-dcoperations) are a powerful and comprehensive means to design and communicate analysis, and are much harder to carry out with tables


```{r}
stars:::get_geotransform(L7_ETMs)
```

# Datacube datasets stored in tables:

## long table:

Each record (row) reflects a unique combination of space (state) and time (year):

```{r}
data(Produc, package = "plm")
head(Produc)
nrow(Produc)
length(unique(Produc$state)) * length(unique(Produc$year))
# but do we acually have all combinations?
with(Produc, length(unique(paste(state, year, sep = ":"))))
```

## space-wide table:

different records (rows) correspond to different times, columns to different locations:
```{r}
data(wind, package = "gstat")
head(wind)
```

## time-wide table:

Different records (rows) correspond to different locations, columns to different times (xxx74: xxx for 1974; xxx79: xxx for 1979)
```{r}
library(sf)
system.file("gpkg/nc.gpkg", package="sf") |> read_sf() -> nc
head(nc)
```

# Creating datacubes from non-datacube data

## Foot-and-mouth disease cases

```{r}
data(fmd, package = 'stpp')
head(fmd)
data("northcumbria", package = 'stpp')
fmd.sf = st_as_sf(as.data.frame(fmd), coords = c('X', 'Y'))
n = nrow(northcumbria)
nh = st_sfc(st_polygon(list(northcumbria[c(1:n,1),])))
plot(fmd.sf, pch = 16, reset = FALSE, extent = nh, breaks = "quantile")
plot(nh, add = TRUE)
```

Create an empty datacube covering the space-time area:
```{r}
library(stars)
st = st_as_stars(nh, nx = 10, ny = 10)
plot(st, reset = FALSE)
plot(nh, add = TRUE, border = 'green')
a = aggregate(fmd.sf, st_as_sf(st), FUN = length)
plot(a, main = "# of cases", reset = FALSE)
plot(fmd.sf, add = TRUE, col = 'grey')
plot(nh, border = 'green', add = TRUE)
```

See https://www.jstatsoft.org/article/view/v053i02 for a more elaborate
approach to model these data statistically.

## Hurricanes

See [hurricanes.Rmd](https://github.com/edzer/OGH23/hurdat.Rmd), [output](hurdat.html)

## OD

See example in SDS: https://r-spatial.org/book/07-Introsf.html#sec-oddc


# Vector data cubes from raster data cubes

Consider the following temperature reanalysis data, taken from [this file](https://psl.noaa.gov/repository/entry/show/PSD+Climate+Data+Repository/Public/PSD+Datasets/PSD+Gridded+Datasets/ncep.reanalysis2.derived/gaussian_grid/skt.sfc.mon.mean.nc?entryid=synth%3Ae570c8f9-ec09-4e89-93b4-babd5651e7a9%3AL25jZXAucmVhbmFseXNpczIuZGVyaXZlZC9nYXVzc2lhbl9ncmlkL3NrdC5zZmMubW9uLm1lYW4ubmM%3D&output=default.html).

We 
```{r}
library(stars)
read_stars('skt.sfc.mon.mean.nc') # GDAL RasterLayer
read_mdim('skt.sfc.mon.mean.nc')  # GDAL Multidimensional Array API
```

Sampling this at two locations:
```{r}
skt = read_stars('skt.sfc.mon.mean.nc')
st_crs(skt) = 'OGC:CRS84'
pts = st_sfc(st_point(c(7, 52)), st_point(c(16.9, 52)), crs = st_crs(skt))
e = st_extract(skt, pts)
library(xts)
e.xts = as.xts(e)
colnames(e.xts) = c("Muenster", "Poznan")
plot(e.xts, legend.loc = "top")
```

Computing the means over three countries:

```{r}
ne = rnaturalearth::ne_countries(returnclass = "sf")
library(dplyr)
sel = c("PL", "DE", "ES")
ne |> filter(iso_a2 %in% sel) -> ne3
aggregate(skt, ne3, FUN=mean, na.rm = TRUE) |>
  aggregate(by = "year", FUN=mean, na.rm = TRUE) |>
  as.xts() -> a3.xts
colnames(a3.xts) = sel
plot(a3.xts, legend.loc = "top")
```
